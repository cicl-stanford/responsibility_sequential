{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment plots\n",
    "\n",
    "Executing this notebook requires reading `.png` files that illustrate the summaries of the experiment's trials. Each trial's summary is given in `experiments/experiment1/trials/{trial_number}/summary.png` and joint summaries of twin trials are given in `experiments/experiment1/trials/{trial_number}/summary_diff.png` of odd trial numbers. For example, `experiments/experiment1/trials/1/summary.png` contains a joint summary of trials 1 and 2. \n",
    "\n",
    "The generated plots can be found in the `figures` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../../code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import json\n",
    "import scipy.stats as stats\n",
    "sns.set_theme(context='paper', style='ticks', font_scale=1)\n",
    "\n",
    "from python import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=\"experiment1\"\n",
    "width_pt = 469\n",
    "seed = 42\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "humancol = \"#762c6c\"\n",
    "aicol = \"#ffa000\"\n",
    "palette = sns.color_palette('husl', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block reads the responsibility judgment data collected from the experiment's participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(\"../\")\n",
    "file = glob.glob(\"./data/{name}/counterfactual_sequential_exp1-trials.csv\".format(name=name))\n",
    "df = pd.read_csv(file[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference of human responsibility per pair of twin trials (Figure 2)\n",
    "\n",
    "The following blocks generate plots that illustrate the difference in responsibility judgments about the human in each pair of twin trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_cf_trials = range(1, 16, 2)\n",
    "diff_df = df[['workerid', 'trial', 'human']].copy()\n",
    "diff_df = diff_df[diff_df['trial'] != 0]\n",
    "diff_df.sort_values(by=['workerid', 'trial'], inplace=True)\n",
    "# make a new column 'CF', if trial odd then 1 else 0,\n",
    "diff_df['CF'] = diff_df['trial'].apply(lambda x: 1 if x in same_cf_trials else 0)\n",
    "# replace every even trial number with the previous trial number\n",
    "diff_df['trial'] = diff_df['trial'].apply(lambda x: x-1 if x not in same_cf_trials else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_ids = diff_df['trial'].unique()\n",
    "\n",
    "for trial in trial_ids:\n",
    "    # filter/transform the dataframe\n",
    "    temp_df = diff_df[diff_df['trial'] == trial].copy()\n",
    "    # add jitter\n",
    "    temp_df['CF_jitter'] = (temp_df['CF']).astype(float) + rng.uniform(-0.1, 0.1, len(temp_df))\n",
    "\n",
    "    utils.latexify() # Computer Modern, with TeX\n",
    "    fig_width, fig_height = utils.get_fig_dim(width_pt, fraction=0.6)\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(fig_width, 0.5*fig_height+fig_width), gridspec_kw={'height_ratios': [fig_width, 0.5*fig_height]})\n",
    "\n",
    "    plt.subplots_adjust(left=0.05, bottom=0.1, right=0.95, top=0.985, wspace=0.0, hspace=0.0)\n",
    "\n",
    "    # draw summary image axis\n",
    "    img = imread('./code/experiments/experiment1/trials/{trial}/summary_diff.png'.format(trial=trial))\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    left, bottom, width, height = axes[1].get_position().bounds\n",
    "    padding = 0.2\n",
    "    new_left = left + padding / 2\n",
    "    new_width = width - padding\n",
    "    axes[1].set_position([new_left, bottom, new_width, height])\n",
    "\n",
    "    # draw plot axis\n",
    "    sns.lineplot(data=temp_df, x='CF_jitter', y='human', hue='workerid', marker='o',\n",
    "                  palette=['lightgray'] * len(temp_df['workerid'].unique()), alpha=.4, linewidth=0.9, legend=False, ax=axes[1], zorder=1)\n",
    "    sns.despine(ax=axes[1])\n",
    "\n",
    "    mean_CFF = temp_df[temp_df['CF'] == 0]['human'].mean()\n",
    "    mean_CFT = temp_df[temp_df['CF'] == 1]['human'].mean()\n",
    "\n",
    "    plt.setp(axes[1].lines, zorder=0)\n",
    "    # add mean points\n",
    "    sns.pointplot(data=temp_df, x='CF', y='human', join=False, capsize=0.1,\n",
    "                  palette=[palette[0], palette[2]], markers=['o', 'o'], ax=axes[1], scale = 1.3)\n",
    "\n",
    "    \n",
    "    axes[1].set_ylim([-5,105])\n",
    "    axes[1].set_xlim([-0.4, 1.4])\n",
    "    if trial == 7:\n",
    "        axes[1].set_ylabel(\"Human responsibility\")\n",
    "    else:\n",
    "        axes[1].set_ylabel(\"\")\n",
    "    axes[1].set_xlabel(\"Counterfactual outcome\")\n",
    "    axes[1].set_xticks([0, 1])\n",
    "    axes[1].set_xticklabels(['Failure', 'Success'])\n",
    "\n",
    "    fig.savefig('figures/experiment1/human_diff_trial_{trial}.pdf'.format(trial=trial), dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of the human's decision quality on responsibility judgments (Figure 3)\n",
    "\n",
    "The following blocks generate a plot that illustrates the distribution of the difference in responsibility judgments $\\Delta_H$ about the human in twin trials where the human made a right and wrong decision, respectively. This is Figure 3(b) in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge every two rows and set human to the difference between row1['human'] and row2['human']\n",
    "diff_df['diff'] = -diff_df.groupby(['workerid', 'trial'])['human'].diff()\n",
    "diff_df.dropna(inplace=True)\n",
    "diff_df.drop(columns=['human', 'CF'], inplace=True)\n",
    "diff_df['right'] = diff_df['trial'].apply(lambda x: 'right' if x in [1,5,9,13] else 'wrong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right:  2.81 (-1.52907883240101, 7.14907883240101)\n",
      "wrong:  11.605 (7.242689165947417, 15.967310834052583)\n"
     ]
    }
   ],
   "source": [
    "utils.latexify() # Computer Modern, with TeX\n",
    "fig_width, fig_height = utils.get_fig_dim(width_pt, fraction=0.45)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(fig_width, fig_height))\n",
    "\n",
    "# draw boxplot of diff for right and wrong trials\n",
    "trick_df = diff_df.copy()\n",
    "# replace right with 1 and wrong with 0\n",
    "trick_df['decision'] = trick_df['right'].replace({'right': 0, 'wrong': 1})\n",
    "trick_df['dummy'] = 0\n",
    "sns.violinplot(data=trick_df, x='diff', hue='decision', ax=ax, palette=[palette[1], palette[3]], inner=None, split=True, orient='h', y='dummy')\n",
    "# show the two means as vertical lines inside the violinplots\n",
    "ax.axvline(x=trick_df[trick_df['decision'] == 1]['diff'].mean(), ymin=0, ymax=0.5, color='#4c4c4c', linestyle='--')\n",
    "ax.axvline(x=trick_df[trick_df['decision'] == 0]['diff'].mean(), ymin=0.5, ymax=1, color='#4c4c4c', linestyle='--')\n",
    "# compute 95% confidence intervals for the two means\n",
    "ci_right = stats.t.interval(0.95, len(trick_df[trick_df['decision'] == 0]['diff'])-1, loc=np.mean(trick_df[trick_df['decision'] == 0]['diff']), scale=stats.sem(trick_df[trick_df['decision'] == 0]['diff']))\n",
    "ci_wrong = stats.t.interval(0.95, len(trick_df[trick_df['decision'] == 1]['diff'])-1, loc=np.mean(trick_df[trick_df['decision'] == 1]['diff']), scale=stats.sem(trick_df[trick_df['decision'] == 1]['diff']))\n",
    "# print the two means and their confidence intervals\n",
    "print(\"right: \", trick_df[trick_df['decision'] == 0]['diff'].mean(), ci_right)\n",
    "print(\"wrong: \", trick_df[trick_df['decision'] == 1]['diff'].mean(), ci_wrong)\n",
    "# add the 95% confidence intervals as ribbons\n",
    "ax.fill_betweenx([0, -0.5], ci_right[0], ci_right[1], color='#4c4c4c', alpha=0.3, linewidth=0.0)\n",
    "ax.fill_betweenx([0, 0.5], ci_wrong[0], ci_wrong[1], color='#4c4c4c', alpha=0.3, linewidth=0.0)\n",
    "\n",
    "sns.despine(ax=ax)\n",
    "\n",
    "ax.set_xlabel(r\"Difference in human responsibility, $\\Delta_H$\")\n",
    "ax.set_ylabel(\"Switching decision\")\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, ['Right', 'Wrong'], title=None, loc='upper right', bbox_to_anchor=(1, 1.1))\n",
    "ax.set_yticks([])\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('figures/experiment1/human_diff_good_bad.pdf', dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following blocks generate a plot that shows the average responsibility attribute to the human and the AI across trials where the human made a right and wrong decision, respectively. This is Figure 3(a) in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_df = df[['workerid', 'trial', 'human', 'ai']].copy()\n",
    "good_df = good_df[good_df['trial']!=0]\n",
    "good_df.sort_values(by=['workerid', 'trial'], inplace=True)\n",
    "good_df['decision'] = good_df['trial'].apply(lambda x: 0 if x in [1,2,5,6,9,10,13,14] else 1)\n",
    "good_df = good_df.melt(id_vars=['workerid','trial','decision'], value_vars=['ai', 'human'], var_name='agent', value_name='resp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.latexify() # Computer Modern, with TeX\n",
    "fig_width, fig_height = utils.get_fig_dim(width_pt, fraction=0.45)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(fig_width, fig_height))\n",
    "\n",
    "# draw barplot with error bars showing the mean ai and human judgments\n",
    "sns.barplot(data=good_df, x='decision', y='resp', hue='agent', ax=ax, palette=[aicol, humancol], errorbar=('ci', 95), alpha=0.7)\n",
    "sns.despine(ax=ax)\n",
    "\n",
    "ax.set_xlabel(\"Switching decision\")\n",
    "ax.set_ylabel(\"Avg. responsibility\")\n",
    "ax.set_xticklabels(['Right', 'Wrong'])\n",
    "ax.set_ylim([0, 115])\n",
    "\n",
    "# set the legend title to 'Agent' and the two colors to 'AI' and 'Human'\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, ['AI', 'Human'], title='Agent', bbox_to_anchor=(0.0, 1.1), loc='upper left', ncol=2)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('figures/experiment1/ai_human_good_bad.pdf', dpi=300)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of responsibility models (Figure 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block reads the logs of the Monte Carlo simulations performed in the grid of each trial and saves to a `.csv` the quantities required for the upcoming analysis.\n",
    "\n",
    "**Requirement:** The code below assumes that the simulations are already performed. Those can be executed using the scripts `generate_counterfactual_decisions.sh`, `generate_counterfactual_episodes.sh` and `count_factual_path_length.sh` found in `code/bash/`. The logs for each grid (`world`) are stored in `code/resources/episodes/{world}/`.\n",
    "\n",
    "**Output:** For each combination of trial, scaler $\\tau$, and scaler $\\theta$, the `csv` contains the counterfactual probability of success without the AI (`prob_succ_no_ai`), the counterfactual probability of success with a different switching decision made by the human (`prob_succ_other_decision`), the probability of making the non-observed switching decision (`prob_other_decision`) and the step count that the human (`human_count`) and the AI (`ai_count`) drove respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read simulation reports\n",
    "worlds=[\"world43\", \"world44\", \"world45\", \"world46\", \"world47\", \"world48\", \"world57\", \"world58\", \"world49\", \"world50\", \"world55\", \"world56\", \"world51\", \"world52\", \"world53\", \"world54\"]\n",
    "trials=[\"trial1\", \"trial2\", \"trial3\", \"trial4\", \"trial5\", \"trial6\", \"trial7\", \"trial8\", \"trial9\", \"trial10\", \"trial11\", \"trial12\", \"trial13\", \"trial14\", \"trial15\", \"trial16\"]\n",
    "simscalers=[0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0]     # this is the policy temperature\n",
    "softscalers=[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0]    # this is the switching decision temperature\n",
    "switch_episodes = [5,6,7,8,13,14,15,16]\n",
    "ai_starts = [1,2,3,4,5,6,7,8]\n",
    "\n",
    "sim_results = []\n",
    "\n",
    "for ind, world in enumerate(worlds):\n",
    "    trial = trials[ind]\n",
    "    for softscaler in softscalers:\n",
    "        for simscaler in simscalers:\n",
    "            trial_num = int(trial.split('trial')[1])\n",
    "            \n",
    "            # read counterfactual episode report\n",
    "            file = glob.glob('code/resources/episodes/{world}/cfreport:{world}_humanscaler:{simscaler}_aiscaler:{simscaler}.json'.format(world=world, simscaler=simscaler))\n",
    "            report = json.load(open(file[0]))\n",
    "            prob_succ_no_ai = np.mean(report['successes'])\n",
    "\n",
    "            results = {\n",
    "                'trial' : trial_num,\n",
    "                'tau_scaler' : simscaler,\n",
    "                'theta_scaler' : softscaler,\n",
    "                'prob_succ_no_ai' : prob_succ_no_ai\n",
    "            }\n",
    "\n",
    "            # read counterfactual decision reports\n",
    "            if trial_num in switch_episodes:\n",
    "                file = glob.glob('code/resources/episodes/{world}/cfnoreport:{world}_humanscaler:{simscaler}_aiscaler:{simscaler}.json'.format(world=world, simscaler=simscaler))\n",
    "                report = json.load(open(file[0]))\n",
    "                prob_succ_no_switch = np.mean(report['successes'])\n",
    "                results['prob_succ_other_decision'] = prob_succ_no_switch\n",
    "            else:\n",
    "                file = glob.glob('code/resources/episodes/{world}/cfyesreport:{world}_humanscaler:{simscaler}_aiscaler:{simscaler}.json'.format(world=world, simscaler=simscaler))\n",
    "                report = json.load(open(file[0]))\n",
    "                prob_succ_switch = np.mean(report['successes'])\n",
    "                results['prob_succ_other_decision'] = prob_succ_switch\n",
    "\n",
    "            # read heuristic step count reports\n",
    "            file = glob.glob('code/resources/episodes/{world}/heurreport:{world}.json'.format(world=world, simscaler=simscaler))\n",
    "            report = json.load(open(file[0]))\n",
    "            results['ai_count'] = report['ai_count']\n",
    "            results['human_count'] = report['human_count']\n",
    "\n",
    "            # read human monte carlo probability estimates reports\n",
    "            file = glob.glob('code/resources/episodes/{world}/humanprobs:{world}_simscaler:{simscaler}_semimanualseed:42.json'.format(world=world, simscaler=simscaler))\n",
    "            report = json.load(open(file[0]))\n",
    "            human_prob = report['human_score']\n",
    "            ai_prob = report['ai_score']\n",
    "            if trial_num in switch_episodes and trial_num in ai_starts:\n",
    "                results['prob_other_decision'] = np.exp(softscaler*ai_prob) / (np.exp(softscaler*ai_prob) + np.exp(softscaler*human_prob))\n",
    "            elif trial_num in switch_episodes and trial_num not in ai_starts:\n",
    "                results['prob_other_decision'] = np.exp(softscaler*human_prob) / (np.exp(softscaler*ai_prob) + np.exp(softscaler*human_prob))\n",
    "            elif trial_num not in switch_episodes and trial_num in ai_starts:\n",
    "                results['prob_other_decision'] = np.exp(softscaler*human_prob) / (np.exp(softscaler*ai_prob) + np.exp(softscaler*human_prob))\n",
    "            elif trial_num not in switch_episodes and trial_num not in ai_starts:\n",
    "                results['prob_other_decision'] = np.exp(softscaler*ai_prob) / (np.exp(softscaler*ai_prob) + np.exp(softscaler*human_prob))\n",
    "            \n",
    "            sim_results.append(results)\n",
    "        \n",
    "sim_df = pd.DataFrame(sim_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the simulation results\n",
    "sim_df.to_csv('code/analysis/files/simulation_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting performed in R\n",
    "\n",
    "Before proceeding to the next block, make sure that the file `code/analysis/files/trial_means.csv` exists. This contains the predictions of responsibility judgments for the human and the AI in each trial, made by (i) simple models relying on the actual contribution of each agent to the outcome, (ii) extensions of models in (i) that also take into account the difficulty of each trial and (iii) our proposed responsibility models.\n",
    "\n",
    "To create the file `trial_means.csv`, one needs to execute the R script `code/analysis/files/model_eval.R` which uses the package `brms` to fit Bayesian Linear Mixed Effects models. It uses optimal values for the scaling parameters $\\tau$ and $\\theta$ selected via grid search (see `code/analysis/files/grid_search.R`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_df = df[df['trial']!=0].copy()\n",
    "\n",
    "# read trial_means.csv\n",
    "trial_means = pd.read_csv('code/analysis/files/trial_means.csv')\n",
    "\n",
    "# join trial_means with reg_df by matching on trial\n",
    "reg_df = reg_df.merge(trial_means, on='trial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate plots of Figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each trial, compute mean and 95% confidence interval of human, ai, heur_human, heur_ai\n",
    "trial_stats = reg_df.groupby('trial').agg({'human': ['mean', lambda x: stats.sem(x) * 1.96], 'ai': ['mean', lambda x: stats.sem(x) * 1.96], 'heur_human': 'mean', 'heur_ai': 'mean'})\n",
    "trial_stats.columns = ['human_mean', 'human_ci', 'ai_mean', 'ai_ci', 'heur_human_mean', 'heur_ai_mean']\n",
    "trial_stats.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Human responsibility: actual contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.latexify() # Computer Modern, with TeX\n",
    "fig_width, fig_height = utils.get_fig_dim(width_pt, fraction=0.4)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(fig_width, fig_width))\n",
    "\n",
    "# draw scatterplot showing the mean ai and human judgments\n",
    "sns.scatterplot(data=trial_stats, x='heur_human_mean', y='human_mean', color=humancol, ax=ax, s=50, alpha=0.5)\n",
    "ax.errorbar(x=trial_stats['heur_human_mean'], y=trial_stats['human_mean'], yerr=trial_stats['human_ci'], fmt='none', color=humancol, capsize=0, alpha=0.3)\n",
    "sns.despine(ax=ax)\n",
    "\n",
    "# draw a line from (0,0) to (100,100)\n",
    "ax.plot([0, 100], [0, 100], color='#4c4c4c', linestyle='--')\n",
    "\n",
    "# compute pearson correlation coefficient\n",
    "r, p = stats.pearsonr(trial_stats['heur_human_mean'], trial_stats['human_mean'])\n",
    "# compute RMSE\n",
    "rmse = np.sqrt(np.mean((trial_stats['heur_human_mean'] - trial_stats['human_mean'])**2))\n",
    "# add the RMSE to the plot\n",
    "ax.annotate(\"RMSE={rmse:.2f}\".format(rmse=rmse), xy=(0.05, 0.9), xycoords='axes fraction', fontsize=11,\n",
    "                bbox=dict(boxstyle='round', fc='white', alpha=0.5))\n",
    "# add the correlation coefficient to the plot\n",
    "ax.annotate(\"r={r:.2f}\".format(r=r), xy=(0.05, 0.8), xycoords='axes fraction', fontsize=11,\n",
    "                bbox=dict(boxstyle='round', fc='white', alpha=0.5))\n",
    "\n",
    "ax.set_xlabel(\"Actual contribution\")\n",
    "ax.set_ylabel(\"Human responsibility\", color=humancol)\n",
    "ax.set_ylim([0, 100])\n",
    "ax.set_xlim([0, 100])\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('figures/experiment1/reg_heur_human.pdf', dpi=300)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AI responsibility: actual contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.latexify() # Computer Modern, with TeX\n",
    "fig_width, fig_height = utils.get_fig_dim(width_pt, fraction=0.4)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(fig_width, fig_width))\n",
    "\n",
    "# draw scatterplot showing the mean ai and human judgments\n",
    "sns.scatterplot(data=trial_stats, x='heur_ai_mean', y='ai_mean', color=aicol, ax=ax, s=50, alpha=0.5)\n",
    "ax.errorbar(x=trial_stats['heur_ai_mean'], y=trial_stats['ai_mean'], yerr=trial_stats['ai_ci'], fmt='none', color=aicol, capsize=0, alpha=0.3)\n",
    "sns.despine(ax=ax)\n",
    "\n",
    "# draw a line from (0,0) to (100,100)\n",
    "ax.plot([0, 100], [0, 100], color='#4c4c4c', linestyle='--')\n",
    "\n",
    "# compute pearson correlation coefficient\n",
    "r, p = stats.pearsonr(trial_stats['heur_ai_mean'], trial_stats['ai_mean'])\n",
    "# compute RMSE\n",
    "rmse = np.sqrt(np.mean((trial_stats['heur_ai_mean'] - trial_stats['ai_mean'])**2))\n",
    "# add the RMSE to the plot\n",
    "ax.annotate(r\"\\textbf{{RMSE={rmse:.2f}}}\".format(rmse=rmse), xy=(0.05, 0.9), xycoords='axes fraction', fontsize=11,\n",
    "                bbox=dict(boxstyle='round', fc='white', alpha=0.5))\n",
    "# add the correlation coefficient to the plot\n",
    "ax.annotate(r\"\\textbf{{r={r:.2f}}}\".format(r=r), xy=(0.05, 0.8), xycoords='axes fraction', fontsize=11,\n",
    "                bbox=dict(boxstyle='round', fc='white', alpha=0.5))\n",
    "\n",
    "ax.set_xlabel(\"Actual contribution\")\n",
    "ax.set_ylabel(\"AI responsibility\", color=aicol)\n",
    "ax.set_ylim([0, 100])\n",
    "ax.set_xlim([0, 100])\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('figures/experiment1/reg_heur_ai.pdf', dpi=300)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each trial, compute mean and std of human and ai\n",
    "trial_stats = reg_df.groupby('trial').agg({'human': ['mean', lambda x: stats.sem(x) * 1.96], 'ai': ['mean', lambda x: stats.sem(x) * 1.96], 'adv_heur_human': 'mean', 'adv_heur_ai': 'mean'})\n",
    "trial_stats.columns = ['human_mean', 'human_ci', 'ai_mean', 'ai_ci', 'adv_heur_human_mean', 'adv_heur_ai_mean']\n",
    "trial_stats.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Human responsibility: actual contribution + difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.latexify() # Computer Modern, with TeX\n",
    "fig_width, fig_height = utils.get_fig_dim(width_pt, fraction=0.4)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(fig_width, fig_width))\n",
    "\n",
    "# draw scatterplot showing the mean ai and human judgments\n",
    "sns.scatterplot(data=trial_stats, x='adv_heur_human_mean', y='human_mean', color=humancol, ax=ax, s=50, alpha=0.5)\n",
    "ax.errorbar(x=trial_stats['adv_heur_human_mean'], y=trial_stats['human_mean'], yerr=trial_stats['human_ci'], fmt='none', color=humancol, capsize=0, alpha=0.3)\n",
    "sns.despine(ax=ax)\n",
    "\n",
    "# draw a line from (0,0) to (100,100)\n",
    "ax.plot([0, 100], [0, 100], color='#4c4c4c', linestyle='--')\n",
    "\n",
    "# compute pearson correlation coefficient\n",
    "r, p = stats.pearsonr(trial_stats['adv_heur_human_mean'], trial_stats['human_mean'])\n",
    "# compute RMSE\n",
    "rmse = np.sqrt(np.mean((trial_stats['adv_heur_human_mean'] - trial_stats['human_mean'])**2))\n",
    "# add the RMSE to the plot\n",
    "ax.annotate(\"RMSE={rmse:.2f}\".format(rmse=rmse), xy=(0.05, 0.9), xycoords='axes fraction', fontsize=11,\n",
    "                bbox=dict(boxstyle='round', fc='white', alpha=0.5))\n",
    "# add the correlation coefficient to the plot\n",
    "ax.annotate(\"r={r:.2f}\".format(r=r), xy=(0.05, 0.8), xycoords='axes fraction', fontsize=11,\n",
    "                bbox=dict(boxstyle='round', fc='white', alpha=0.5))\n",
    "\n",
    "ax.set_xlabel(\"Actual contribution + difficulty\")\n",
    "ax.set_ylabel(\"Human responsibility\", color=humancol)\n",
    "ax.set_ylim([0, 100])\n",
    "ax.set_xlim([0, 100])\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('figures/experiment1/reg_adv_heur_human.pdf', dpi=300)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each trial, compute mean and std of human and ai\n",
    "trial_stats = reg_df.groupby('trial').agg({'human': ['mean', lambda x: stats.sem(x) * 1.96], 'ai': ['mean', lambda x: stats.sem(x) * 1.96], 'cf_full_human': 'mean', 'cf_full_ai': 'mean'})\n",
    "trial_stats.columns = ['human_mean', 'human_ci', 'ai_mean', 'ai_ci', 'cf_full_human_mean', 'cf_full_ai_mean']\n",
    "trial_stats.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Human responsibility: our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.latexify() # Computer Modern, with TeX\n",
    "fig_width, fig_height = utils.get_fig_dim(width_pt, fraction=0.4)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(fig_width, fig_width))\n",
    "\n",
    "# draw scatterplot showing the mean ai and human judgments\n",
    "sns.scatterplot(data=trial_stats, x='cf_full_human_mean', y='human_mean', color=humancol, ax=ax, s=50, alpha=0.5)\n",
    "ax.errorbar(x=trial_stats['cf_full_human_mean'], y=trial_stats['human_mean'], yerr=trial_stats['human_ci'], fmt='none', color=humancol, capsize=0, alpha=0.3)\n",
    "sns.despine(ax=ax)\n",
    "\n",
    "# draw a line from (0,0) to (100,100)\n",
    "ax.plot([0, 100], [0, 100], color='#4c4c4c', linestyle='--')\n",
    "\n",
    "# compute pearson correlation coefficient\n",
    "r, p = stats.pearsonr(trial_stats['cf_full_human_mean'], trial_stats['human_mean'])\n",
    "# compute RMSE\n",
    "rmse = np.sqrt(np.mean((trial_stats['cf_full_human_mean'] - trial_stats['human_mean'])**2))\n",
    "# add the RMSE to the plot\n",
    "ax.annotate(r\"\\textbf{{RMSE={rmse:.2f}}}\".format(rmse=rmse), xy=(0.05, 0.9), xycoords='axes fraction', fontsize=11,\n",
    "                bbox=dict(boxstyle='round', fc='white', alpha=0.5), weight='bold')\n",
    "# add the correlation coefficient to the plot\n",
    "ax.annotate(r\"\\textbf{{r={r:.2f}}}\".format(r=r), xy=(0.05, 0.8), xycoords='axes fraction', fontsize=11,\n",
    "                bbox=dict(boxstyle='round', fc='white', alpha=0.5), weight='bold')\n",
    "\n",
    "ax.set_xlabel(\"Our model\")\n",
    "ax.set_ylabel(\"Human responsibility\", color=humancol)\n",
    "ax.set_ylim([0, 100])\n",
    "ax.set_xlim([0, 100])\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('figures/experiment1/reg_cf_full_human.pdf', dpi=300)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AI responsibility: our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.latexify() # Computer Modern, with TeX\n",
    "fig_width, fig_height = utils.get_fig_dim(width_pt, fraction=0.4)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(fig_width, fig_width))\n",
    "\n",
    "# draw scatterplot showing the mean ai and human judgments\n",
    "sns.scatterplot(data=trial_stats, x='cf_full_ai_mean', y='ai_mean', color=aicol, ax=ax, s=50, alpha=0.5)\n",
    "ax.errorbar(x=trial_stats['cf_full_ai_mean'], y=trial_stats['ai_mean'], yerr=trial_stats['ai_ci'], fmt='none', color=aicol, capsize=0, alpha=0.3)\n",
    "sns.despine(ax=ax)\n",
    "\n",
    "# draw a line from (0,0) to (100,100)\n",
    "ax.plot([0, 100], [0, 100], color='#4c4c4c', linestyle='--')\n",
    "\n",
    "# compute pearson correlation coefficient\n",
    "r, p = stats.pearsonr(trial_stats['cf_full_ai_mean'], trial_stats['ai_mean'])\n",
    "# compute RMSE\n",
    "rmse = np.sqrt(np.mean((trial_stats['cf_full_ai_mean'] - trial_stats['ai_mean'])**2))\n",
    "# add the RMSE to the plot\n",
    "ax.annotate(\"RMSE={rmse:.2f}\".format(rmse=rmse), xy=(0.05, 0.9), xycoords='axes fraction', fontsize=11,\n",
    "                bbox=dict(boxstyle='round', fc='white', alpha=0.5))\n",
    "# add the correlation coefficient to the plot\n",
    "ax.annotate(\"r={r:.2f}\".format(r=r), xy=(0.05, 0.8), xycoords='axes fraction', fontsize=11,\n",
    "                bbox=dict(boxstyle='round', fc='white', alpha=0.5))\n",
    "\n",
    "ax.set_xlabel(\"Our model\")\n",
    "ax.set_ylabel(\"AI responsibility\", color=aicol)\n",
    "ax.set_ylim([0, 100])\n",
    "ax.set_xlim([0, 100])\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('figures/experiment1/reg_cf_full_ai.pdf', dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e312f55c3c97e43ef328a86651ae8cf61ffd8a19c7888f1a2843b6f17cf5ecdd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('env')",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "c425b3d18afb7ef01beae38734bbc4c3f66a0926b3e7f0a803887fb6dc8ff96a"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
